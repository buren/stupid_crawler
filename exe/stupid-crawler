#!/usr/bin/env ruby
require 'optparse'
require 'uri'

require 'stupid_crawler'

site = nil
max_urls = Float::INFINITY
sleep_time = 0.1
robots = false
ignore_links = nil
dir_path = nil

optparse = OptionParser.new do |parser|
  parser.on('--site=example.com', String, 'The site to crawl') do |value|
    site = value
  end

  parser.on('--max=10000', Integer, 'Max number of URLs to crawl (default: Infinity)') do |value|
    max_urls = value
  end

  parser.on('--sleep=0.1', Integer, 'Sleep time between URL fetches (default: 0.1)') do |value|
    sleep_time = value
  end

  parser.on('--ignore-links=/blog/', String, 'Ignore links matching this pattern (default: none)') do |value|
    ignore_links = value
  end

  parser.on('--output=/result/', String, 'Output directory (default:)') do |value|
    dir_path = value
  end

  parser.on('--[no-]robots', "Respect robots.txt (default: false)") do |value|
    robots = value
  end

  parser.on('-h', '--help', 'How to use') do
    puts parser
    exit
  end
end

optparse.parse!

if site.nil? || site.empty?
  raise OptionParser::MissingArgument, "'--site' can't be blank"
end

unless site.start_with?('http://') || site.start_with?('https://')
  site = "http://#{site}"
end

result = StupidCrawler::Crawler.new(
  site,
  max_urls: max_urls,
  sleep_time: sleep_time,
  robots: robots,
  ignore_links: ignore_links
).call

dir_path = URI.parse(site).host if dir_path.nil?
StupidCrawler::ResultWriter.call(result, dir_path: dir_path)
